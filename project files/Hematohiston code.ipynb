{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP+3L8yhbNdJhDHXknNF70B"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Install Gradio\n","!pip install -q gradio\n","\n","# Imports\n","import os\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications import EfficientNetB0\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.callbacks import EarlyStopping\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","import matplotlib.pyplot as plt\n","import cv2\n","import random\n","from PIL import Image\n","import gradio as gr\n","\n","# Constants\n","IMG_SIZE = (224, 224)\n","BATCH_SIZE = 32\n","EPOCHS = 5\n","CLASSES = ['Neutrophil', 'Eosinophil', 'Basophil', 'Lymphocyte', 'Monocyte']\n","NORMAL_CLASSES = ['Neutrophil', 'Lymphocyte']\n","NUM_CLASSES = len(CLASSES)\n","\n","# Step 1: Create synthetic blood image dataset\n","def create_blood_dataset():\n","    for split in ['train', 'val', 'test']:\n","        for cls in CLASSES:\n","            os.makedirs(f\"blood_dataset/{split}/{cls}\", exist_ok=True)\n","\n","    colors = {\n","        'Neutrophil': [255, 0, 0],\n","        'Eosinophil': [0, 255, 0],\n","        'Basophil': [0, 0, 255],\n","        'Lymphocyte': [255, 255, 0],\n","        'Monocyte': [255, 0, 255]\n","    }\n","\n","    for cls in CLASSES:\n","        for split, count in zip(['train', 'val', 'test'], [50, 15, 20]):\n","            for i in range(count):\n","                img = np.zeros((224, 224, 3), dtype=np.uint8)\n","                img[:, :, :] = colors[cls]\n","                cv2.putText(img, cls[0], (80, 130), cv2.FONT_HERSHEY_SIMPLEX, 2, (255,255,255), 3)\n","                cv2.imwrite(f\"blood_dataset/{split}/{cls}/{cls}_{i}.jpg\", img)\n","\n","create_blood_dataset()\n","\n","# Step 2: Load datasets\n","train_gen = ImageDataGenerator(rescale=1./255).flow_from_directory(\n","    'blood_dataset/train', target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical')\n","\n","val_gen = ImageDataGenerator(rescale=1./255).flow_from_directory(\n","    'blood_dataset/val', target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical')\n","\n","test_gen = ImageDataGenerator(rescale=1./255).flow_from_directory(\n","    'blood_dataset/test', target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', shuffle=False)\n","\n","labels = list(train_gen.class_indices.keys())\n","print(f\"\\nâœ… Classes detected: {train_gen.num_classes}\")\n","\n","# Step 3: Build model\n","base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=(224,224,3))\n","base_model.trainable = False\n","\n","model = models.Sequential([\n","    base_model,\n","    layers.GlobalAveragePooling2D(),\n","    layers.Dense(128, activation='relu'),\n","    layers.Dropout(0.3),\n","    layers.Dense(NUM_CLASSES, activation='softmax')\n","])\n","\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Step 4: Train model\n","history = model.fit(train_gen, validation_data=val_gen, epochs=EPOCHS,\n","                    callbacks=[EarlyStopping(patience=2, restore_best_weights=True)])\n","\n","# Save model\n","model.save('hematovision_model.h5')\n","\n","# Step 5: Confusion Matrix\n","y_true = test_gen.classes\n","y_probs = model.predict(test_gen)\n","y_pred = np.argmax(y_probs, axis=1)\n","cm = confusion_matrix(y_true, y_pred)\n","\n","plt.figure(figsize=(6,6))\n","disp = ConfusionMatrixDisplay(cm, display_labels=labels)\n","disp.plot(cmap=\"Blues\", values_format=\"d\")\n","plt.title(\"Confusion Matrix\")\n","plt.savefig(\"confusion_matrix.png\")\n","plt.close()\n","\n","# Step 6: Accuracy & Loss Plots\n","plt.figure(figsize=(10, 4))\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['accuracy'], label=\"Train\")\n","plt.plot(history.history['val_accuracy'], label=\"Val\")\n","plt.title(\"Accuracy\")\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['loss'], label=\"Train\")\n","plt.plot(history.history['val_loss'], label=\"Val\")\n","plt.title(\"Loss\")\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.savefig(\"metrics_plot.png\")\n","plt.close()\n","\n","# Step 7: Sample predictions - Mixed (Normal & Abnormal)\n","sample_paths = []\n","# 3 Normal\n","for cls in NORMAL_CLASSES:\n","    cls_path = f\"blood_dataset/test/{cls}\"\n","    img_file = random.choice(os.listdir(cls_path))\n","    sample_paths.append(f\"{cls_path}/{img_file}\")\n","\n","# 3 Abnormal\n","for cls in set(CLASSES) - set(NORMAL_CLASSES):\n","    cls_path = f\"blood_dataset/test/{cls}\"\n","    img_file = random.choice(os.listdir(cls_path))\n","    sample_paths.append(f\"{cls_path}/{img_file}\")\n","\n","# Step 8: Prediction function with Normal/Abnormal\n","def predict_and_plot(img_path):\n","    img = Image.open(img_path).resize(IMG_SIZE)\n","    img_arr = np.expand_dims(np.array(img)/255.0, axis=0)\n","    probs = model.predict(img_arr)[0]\n","    pred_idx = np.argmax(probs)\n","    pred_label = labels[pred_idx]\n","    conf = probs[pred_idx]\n","    status = \"Normal\" if pred_label in NORMAL_CLASSES else \"Abnormal\"\n","\n","    plt.figure(figsize=(4, 6))\n","    plt.imshow(img)\n","    plt.title(f\"{pred_label} ({conf:.2f}) - {status}\", fontsize=12)\n","    plt.axis(\"off\")\n","    out_path = f\"pred_{os.path.basename(img_path)}\"\n","    plt.savefig(out_path)\n","    plt.close()\n","    return out_path\n","\n","# Step 9: Gradio UI\n","with gr.Blocks() as demo:\n","    gr.Markdown(\"## ğŸ©¸ Hematovision: Blood Cell Classification Demo\")\n","\n","    with gr.Row():\n","        with gr.Column():\n","            gr.Markdown(\"### ğŸ”¬ Confusion Matrix\")\n","            gr.Image(value=\"confusion_matrix.png\", label=\"Confusion Matrix\")\n","\n","        with gr.Column():\n","            gr.Markdown(\"### ğŸ“ˆ Accuracy and Loss\")\n","            gr.Image(value=\"metrics_plot.png\", label=\"Accuracy & Loss\")\n","\n","    gr.Markdown(\"### ğŸ§ª Sample Predictions (Mixed Normal & Abnormal)\")\n","\n","    for path in sample_paths:\n","        gr.Image(value=predict_and_plot(path), label=os.path.basename(path))\n","\n","demo.launch()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"yVc-lpJBs6kF","executionInfo":{"status":"ok","timestamp":1751150703591,"user_tz":-330,"elapsed":186884,"user":{"displayName":"poojitha sri anjali sama | AP23110020114","userId":"06291439492010826674"}},"outputId":"23023a66-9881-4251-bc3d-cdbefd35d694"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 250 images belonging to 5 classes.\n","Found 75 images belonging to 5 classes.\n","Found 100 images belonging to 5 classes.\n","\n","âœ… Classes detected: 5\n","Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n","\u001b[1m16705208/16705208\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Epoch 1/5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.2159 - loss: 1.6857"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 4s/step - accuracy: 0.2128 - loss: 1.6857 - val_accuracy: 0.2000 - val_loss: 1.6324\n","Epoch 2/5\n","\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3s/step - accuracy: 0.2001 - loss: 1.6913 - val_accuracy: 0.2000 - val_loss: 1.6203\n","Epoch 3/5\n","\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - accuracy: 0.1952 - loss: 1.6169 - val_accuracy: 0.2000 - val_loss: 1.6102\n","Epoch 4/5\n","\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - accuracy: 0.2026 - loss: 1.6084 - val_accuracy: 0.2000 - val_loss: 1.6159\n","Epoch 5/5\n","\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3s/step - accuracy: 0.1814 - loss: 1.6230 - val_accuracy: 0.2000 - val_loss: 1.6079\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step\n","\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n","\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n","It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://2477270fd355aefe39.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://2477270fd355aefe39.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":1},{"output_type":"display_data","data":{"text/plain":["<Figure size 600x600 with 0 Axes>"]},"metadata":{}}]}]}